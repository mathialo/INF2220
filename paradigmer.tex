\section{Paradigmer for algoritmedesign}

\subsection{Splitt og hersk}
Splitt og hersk er en teknikk som i stor grad benytter seg av rekursjon. Vi deler problemet opp i mindre delproblemer, deler de delproblemene opp i mindre deldelproblemer også videre. Slik fortsetter vi helt til problemene er så små at løsningen er triviell. Deretter setter vi sammen løsningen på småproblemene til en løsning på hele problemet. 

Eksempler på algoritmer som bruker denne teknikken er søking i binære trær (se \ref{bintraer}) og Quicksort (\ref{quick}).


\subsection{Grådige algoritmer}
Grådige algoritmer er algoritmer som løser optimeringsproblemer. En grådig algoritme vil gå steg for steg gjennom problemet, og gjøre det som ser best ut på hvert tidspunkt.

Eksempler på grådige algoritmer er Dijkstras algoritme (se \ref{dijkstra}), Prims algoritme (\ref{prim}), Kruskals algoritme (\ref{kruskal}) og Huffmankoding (\ref{huffman}).



\subsection{Dynamisk programmering}
Dynamisk programmering er en designteknikk som går ut på å forsøke å gjøre komplekse optimeringsproblemer enklere ved å dele problemet opp i mindre delproblemer, og løse dem hver for seg. Vi lagrer løsningene, og bruker resultatet fra dem til å konstruere med en endelig løsning. Prinsippet går ut på at en optimal løsning på hele problemet vil være et resultat av optimale løsninger på delproblemene. Det er ikke alltid tilfelle, men når det er det kan dynamisk programmering forbedre kjøretiden dramatisk.

For illustrere tankegangen skal vi se på et eksempel. Fibonaccitallene er definert rekursivt slik:
\[ f(n) =	\left\{
	\begin{array}{ll}
		1 \quad& \text{for } n \in \{1, 2\} \\
		f(n-1) + f(n-2) \quad& \text{ellers} 
	\end{array}
	\right. \]
Vi skal programmere en funksjon som regner ut $ f(n) $. Det er fristende å gjøre det helt likt som definisjonen:
\javaimport{fib_rek.java}
Dette er en grei og oversiktlig implementasjon, i den forstand at den finner tallet vi ber den om, men la oss foreta en liten tidsanalyse. For hvert tall vi ber den om må den regne ut to tall. For hver av disse to tallene må vi igjen regne ut to tall. Slik baller det på seg. Vi kan tegne opp et tre over funksjonskallene:

\begin{figure}[H]
\caption{Funksjonskall for $ f(5) $}
\label{fig:fib_rekursivt}
\centering
~\\
\begin{tikzpicture}[level distance=1.5cm,
  level 1/.style={sibling distance=3.5cm},
  level 2/.style={sibling distance=2.5cm},
  level 3/.style={sibling distance=1.5cm},
every node/.style = {align=center}]

\node{$ f(5) $}
	child {node {$ f(4) $}
		child {node {$ f(2) $}}
		child {node {$ f(3) $}
			child {node {$ f(2) $}}
			child {node {$ f(1) $}}
		}
	}
	child {node {$ f(3) $}
		child {node {$ f(2) $}}
		child {node {$ f(1) $}}
	};
\end{tikzpicture}
\end{figure}

Vi ser at vi vil regne ut $ f(3) $ to ganger, det virker litt overflødig. Generelt vil denne algoritmen bruke $ O(2^n) $ tid, som er veldig dårlig. Hvis vi prøver å tenke dynamisk kan vi løse problemet mye bedre. I stedet for å programmere fibonaccifunksjonen vår rekursivt vil vi gjøre det iterativt, og hvor vi lagrer løsningene underveis. Da kan vi, i stedet for å regne ut de foregående tallene, finne dem i en tabell. 
\javaimport{fib_dyn.java}
Denne koden er kanskje litt mindre intuitiv enn den forrige, men den er mye raskere! Her ser vi at algoritmen kun har én loop, og kjøretiden er opplagt $ O(n) $.

Et eksempel på en algoritme som bruker denne teknikken er Floyds algoritme (se \ref{floyd}).




\subsection{Kombinatorisk søk}
Kombinatorisk søking er en strategi vi bruker når vi ikke har noen andre muligheter. Strategien går i hovedsak ut på å teste alle muligheter. Dette er veldig tregt, og har ofte eksponentiell tid. Når det er sagt kan vi ofte gjøre visse \textbf{avskjæringer} for å forbedre tiden. Avskjæringer er når vi kan fjerne mange muligheter fordi vi veit at svaret ikke finnes der uansett. 

Eksempler på problemer vi må bruke kombinatorisk søk på er TSP, subset sum eller å plassere åtte dronninger på et skjakkbrett uten at noen står i slag.