\section{Kompleksitet og tidsanalyse}
\subsection{Terminologi og begreper}
\subsubsection{Alfabeter og språk}
Når vi bruker begrepene alfabet og språk snakker vi som regel ikke om språk som engelsk, norsk eller Java (selv om disse også er språk i formell forstand), men om en samling strenger av tegn (tenk: ord). Hvilke tegn vi kan bruke avhenger av hvilket alfabet vi har.

Et \textbf{alfabet} er en ikke-tom mengde av tegn (også kalt symboler og bokstaver). Vi betegner ofte et alfabet med $ \Sigma $. Eksempler på alfabeter kan være det binære alfabetet $ \{0, 1\} $ eller det norske alfabetet \{a, b, c, d, e, f, g, h, i, j, k, l,  m, n, o, p, q, r, s, t, u, v, w, x, y, z, æ, ø, å\}. \index{alfabet}

Gitt et alfabet $ \Sigma $ bruker vi $ \Sigma ^* $ for å betegne mengden av alle mulige kombinasjoner (strenger) av tegn fra $ \Sigma $ av en endelig lengde. En mengde av strenger, med eller uten noen bestemte regler, kalles et \textbf{språk}. Vi konstruerer språk enten ved å liste opp alle ordene i språket, eller ved å gi noen regler som ordene i språket må følge. Under følger noen eksempler på språk, og hvordan de defineres:\index{språk}
\begin{itemize}
\item $ L = \{\say{a}, \say{b}, \say{ab}, \say{ba}\}$ (eksempel på et endelig språk)
\item $ L = \Sigma^* $ (alle ordene over $ \Sigma $) (eksempel på et uendelig språk)
\item $ L = \{i : M \text{ stopper på input } i \} $ (Den inputen som gjør at en turingmaskin stopper. En variant av haltingproblemet, se \ref{haltingProof})\index{haltingproblemet}
\end{itemize}
\label{språk}
\begin{example} Gitt alfabetet $ \{0, 1, \text{`,'}\} $, hva er det formelle språket som tilsvarer sorteringsproblemet over dette alfabetet?

\emph{Svar:}
\[ L = \{(0), (1), ..., (0,1), (0, 10), (1, 10), ...\} \]

\end{example}

\subsubsection{Turingmaskinen}
Turingmaskinen er en teoretisk maskin. Den eksisterer ikke i virkeligheten, men er noe vi bruker for å bevise teoremer. Selvfølgelig finnes det ingen formell definisjon på \emph{hva} en Turingmaskin egentlig er, så alle læreverk definerer den litt forskjellig. Det høres kanskje helt Texas ut, men alle definisjonene er tilnærmet ekvivalente. \index{Alan Turing}\index{turingmaskin}

\begin{definition}
\label{def:turingmaskin}
Litt forenklet kan vi si at en Turingmaskin $ M = (\Sigma, \Gamma, Q, \delta) $ består av fire komponenter:
\begin{itemize}
\item Et inputalfabet $ \Sigma $. Alle mulige tegn Turingmaskinen kan forstå.
\item Et teipalfabet $ \Gamma $. Alle mulige tegn som kan finnes på teipen. $ \Sigma $ er alltid inneholdt i $ \Gamma $. $ \Gamma $ inneholder også et blankt symbol (mellomrom), og kan inneholde andre symboler. 
\item En liste $ Q $ over mulige statuser for Turingmaskinen.
\item En liste $ \delta $ over `responser' på input. For eksempel: \say{Hvis jeg er i status 7 og leser en `a' skal jeg gå til status 21}.
\end{itemize}
\end{definition}

Hvis vi skal prøve å se for oss en Turingmaskin intuitivt kan vi se for oss et uendelig lang papirremse (formelt kalt teip). På denne papirremsa står det symboler (fra $ \Sigma $). Dette er inputen til Turingmaskinen. Maskinen leser ett og ett symbol, og reagerer på symbolet (etter hva $ \delta $ sier den skal gjøre). Det kan deretter la symbolet stå, viske det vekk eller erstatte det med et nytt symbol (fra $ \Gamma $).


\subsection{Kompleksitetsklasser}

Noen problemer kan løses veldig enkelt, for eksempel som å søke i et binært søketre, andre problemer er mye mer kompliserte, og noen er uløselige. I dette kurset skiller vi i hovedsak mellom P, NP og uløselige problemer, selv om kompleksitetsklasser er mye finere oppdelt enn det. 

En av grunnene til at vi deler opp problemer i kompleksitetsklasser er at vi på forhånd kan si noe om løsbarheten til problemene, før vi begynner å løse dem. En annen grunn er at problemer i samme klasse kan ha løsninger som ligner, selv om problemene er ganske ulike. Se kapitlet om paradigmer (\ref{paradigmer}). For eksempel er både søk i binærtre og quicksort eksempler på splitt og hersk-algoritmer. De ligner i utforming, men gjør ganske forskjellige ting. Begge de to algoritmene ligger i P. 



\paragraph{P (polynomial time)}
P er mengden av alle problemene som kan løses i polynomisk tid, altså har tid på formen $ O(n^k) $ for en $ k \in \mathbb{N} $

\paragraph{NP (nondeterministic polynomial time)}
NP kan defineres på flere måter. En enkel måte å tenke om NP på er at NP er alle problemene som kan være vanskelige å løse, men hvor en løsning kan sjekkes i polynomisk tid. For eksempel kan sudoku være veldig vanskelig å løse, men å skjekke at en løsning er rett er ganske enkelt, det er bare å skjekke at alle rader, kolonner og bokser er riktig utfylt (ikke inneholder en verdi mer enn 1 gang). 

Siden alle problemer i P kan skjekkes i polynomisk tid (man kan bare løse problemet på nytt å se om man får samme svar) er det klart at P $ \subseteq $ NP. Det er faktisk ikke helt klart hvorvidt NP $ \subseteq $ P også, dvs om det faktisk er en reell forskjell på P og NP. Dette er faktisk et av millenniumsproblemene, og et av de store uløste problemene i matematikk. Det er verdt å nevne at de fleste tror at P $ \neq $ NP, det mangler bare et formelt bevis.

Alle problemene som er like vanskelig eller vanskeligere enn alle problemene i NP kalles NP-hard. De problemene som er i både NP-hard og NP er dermed de vanskeligste problemene i NP. Denne mengden kalles NP-komplett. 

\paragraph{EXPTIME (exponetial time)}
EXPTIME er mengden av alle problemer som løses og sjekkes i eksponentiell tid. Hvis jeg for eksempel ber deg fortelle meg hva det beste trekket jeg burde gjøre i et parti sjakk er, er det vanskelig for deg å regne ut, men også vanskelig for meg å sjekke om svaret ditt stemmer. I mange tilfeller er brute force den eneste muligheten vi har. 

På samme måte som for NP har vi EXPTIME-hard og EXPTIME-komplett. Det å avgjøre hvilket trekk i et parti sjakk som er det beste er et EXPTIME-komplett . 

\paragraph{R}
R er mengden av problemer som er løselig i endelig tid. De kan ta flere ganger av alderen til universet å komme til en løsning, men det er mulig. 

\paragraph{Uløselige problemer} 
En del problemer har ikke en mulig løsning. Et eksempel på et slikt problem er halting-problemet. Dette problemet går ut på om en turingmaskin kan vite om den noen gang vil gi et resultat (det vil si stoppe, engelsk: halt), eller om den vil gå i evig loop. Et bevis for hvorfor haltingproblemet er uløselig finnes i \ref{haltingProof}. 

~\\
\noindent Vi skal se på noen eksempler på problemer:

\begin{example}
Traveling salesperson (TSP)

Et av de mest kjente og studerte optimeringsproblemene kalles \emph{Traveling salesperson}. Problemet går slik: En handelsmann har en liste med byer han må innom. Han vil reise innom hver by én gang, og vil bruke minst mulig penger på turen. Han vet prisen det vil koste å reise mellom hver by. Hvilken rekkefølge burde handelsmannen besøke byene i for å betale minst mulig i reisepenger?

Dette problemet er av eksponentiell karakter. Vi kan ikke si noe om hvilken vei som blir billigst uten å sjekke alle muligheter. Problemet er NP-komplett.
\end{example}


\begin{example}
Subset sum

Problemet er slik: Gitt en mengde $ M $ av tall, skal vi avjøre om det finnes en delmengde $ M' \subset M $ slik at  \[ \sum_{m~\in~M'} m = 0  \] Mengden $ A =  \{-2, -3, 1, 4, 6\} $ er en slik mengde, siden $ 1 + 4 + (-2) + (-3) = 0 $. Mengden $ B = \{-6, -3, 1, 4, 7\} $ er ikke en slik mengde, siden det ikke er mulig å plukke ut noen elementer slik at summen av elementene blir 0.

Å løse dette problemet er ganske vanskelig siden det er NP-komplett. Vi må (slik som i TSP) prøve oss fram med forskjellige kombinasjoner. Å skjekke om en antatt delmengde har sum $ = 0 $ er enkelt: det er bare å summere og skjekke, og kan gjøres i $ O(n) $ tid. 
\end{example}



\subsection{O-notasjon}
Når vi skal analysere kjøretid er vi sjeldent opptatt av et nøyaktig svar, men mer opptatt av hva slags størrelsesorden kjøretiden befinner seg i. Dette er litt av motivasjonen for O-notasjon. Formelt kan vi definere det slik:

Hvorfor big O? Vi er interessert i å vite noe om generell tid, fordi faktisk tid vil avhenge av maskin etc. 

\begin{definition}
\label{def:O}
La $ f $ og $ g $ være to funksjoner $ f, g:\mathbb{N} \rightarrow \mathbb{R} $. Vi sier da at $ f(n) = O(g(n))$ hvis det eksisterer positive heltall $ c $ og $ N $ slik at for hvert heltall $ n\geq N $ er $ f(n) \leq c\,g(n) $
\end{definition}
$ O(g(n)) $ blir dermed en øvre skranke for kjøretid.

Når vi i denne sammenhengen bruker O-notasjon vil vi bruke det som et mål på hvordan kjøretiden øker med inputen. Vi ser på et eksempel:

\begin{example}
Vi er gitt en array \mono{int a[]}, med $ n $ elementer, og skal summere alle elementene i \mono{a[]}. Det kan gjøres slik:
\begin{lstlisting}[language=Java,
commentstyle=\color{source_brown}\monofontitalic, 
morekeywords={String},
keywordstyle=\color{source_blue}\monofontbold,
stringstyle=\color{source_orange}]
int sum = 0;

for (int i=0; i<n; i++) {
	sum += a[i];
}
\end{lstlisting}
Vi lurer nå på, hva er kjøretiden til denne algoritmen? Vi ser at vi gjør to forskjellige operasjoner her: oppretter en variabel, og plusser sammen to tall. Den siste operasjonen gjør vi $ n $ ganger på grunn av løkka.

Vi har dermed at kjøretiden, $ T(n) $, blir:
\[ T(n) = t_{\text{opprett variabel}} + n \cdot t_{\text{pluss}}  \]
der $ t_{\text{opprett variabel}} $ angir tiden det tar å opprette en variabel i minne, og $ t_{\text{pluss}} $ angir tiden det tar å plusse sammen to tall. Problemet er at disse konstantene varierer ut fra hvilken datamaskin vi bruker, hvilket språk vi implementerer i, etc. Det eneste vi vet noe om er at tiden øker lineært med størrelsen. Derfor er 
\[ T(n) = O(n) \]
\end{example}

Som nevnt tidligere er vi mer opptatt av størrelsesorden enn den konkrete kjøretiden. Vi bryr oss derfor ikke om konstanter. Hvis vi i eksempelet hadde måttet gjøre 2 tester for hver input hadde vi fortsatt hatt $ O(n) $ tid, selv om kjøretiden hadde vært $ T(n) = 2n $. Vi kan sette opp noen regneregler for O-notasjon:
\begin{theorem}
\label{teo:O}
Regneregler for O-notasjon:
\begin{enumerate}[i]
	\item Hvis $ T(n) = c\cdot f(n) $ for en konstant $ c $ og en funksjon $ f $, så er $ T(n) = O(f(n)) $
	\item Hvis $ T(n) = f(n) + g(n) $ for to funksjoner $ f $ og $ g $, og det finnes et tall $ N $ slik at $ f(n) > g(n) $ for alle $ n > N$ , så er $ T(n) = O(f(n)) $
\end{enumerate}
\end{theorem}

Dette er ikke veldig storlagne resultater, de følger egentlig direkte fra definisjonen. Det del \textit{i} egentlig sier er at vi kan droppe konstanter når vi jobber med O-notasjon. Vi vil aldri se uttrykk som $ O(4n^2) $, vi skriver bare $ O(n^2) $. Det er fordi at forskjellen mellom $ 4n^2 $ og $ n^2 $ er så liten i forhold til forskjellen mellom $ n^2 $ og $ n^3 $. 

Del \textit{ii} sier at vi bare trenger å se på den største funksjonen. Vi vil for eksempel aldri skrive $ O(n^2 + n) $, vi vil bare skrive $ O(n^2) $ siden $ n $ blir så liten i forhold til $ n^2 $.


\begin{example} Beregning av kjøretid. Vi har gitt følgende program
\begin{lstlisting}[language=Java,
commentstyle=\color{source_brown}\monofontitalic, 
morekeywords={String},
keywordstyle=\color{source_blue}\monofontbold,
stringstyle=\color{source_orange}]
for (int i=0; i<n; i++) {
    for (int j=i; j<n; j++) {
        // Do something simple...
    }
}

for (int i=0; i<n-3; i++) {
    // Do something else...
}
\end{lstlisting}
og skal beregne worst case kjøretid til programmet. Vi ser at den indre for-løkka i den øverste for-løkka starter på \mono{i}, og ikke på 0. Fra teorem \ref{teo:O} del i har vi at det ikke har noe å si. Vi regner med den løkka. Vi ser også at det er en enkel for-løkke etterpå, men fra teorem \ref{teo:O} del ii har vi at vi kan se bort fra den. Kjøretiden blir altså $ O(n^2) $.
\end{example}


\begin{example} Finn kjøretid for følgende program: (Ex14 1b)

\begin{lstlisting}[language=Java,
commentstyle=\color{source_brown}\monofontitalic, 
morekeywords={String},
keywordstyle=\color{source_blue}\monofontbold,
stringstyle=\color{source_orange}]
for (i=n; i >= 1; i = i/2) {
    for (j=1; j<n; j++) {
        // do something
    }
}
\end{lstlisting}
Vi ser at den indre løkka vil gå $ n $ ganger. Den ytre løkka halverer telleren hver gang, det vil si at den kjører $ \log_2 n $ ganger. Kjøretiden blir derfor $ O(n\log_2 n) $.
\end{example}

\begin{example} Finn kjøretid for følgende program: (Ex11 2b)
\begin{lstlisting}[language=Java,
commentstyle=\color{source_brown}\monofontitalic, 
morekeywords={String},
keywordstyle=\color{source_blue}\monofontbold,
stringstyle=\color{source_orange}]
float foo(A) {
    n = A.length;
    
    if (n==1) {
        return A[0];
    }
    
    // let A1, A2, A3 and A4 be arrays of length n/2
    
    for (i=0; i<=n/2; i++) {
        for (j=0; j<=n/2; j++) {
            A1[i] = A[i];
            A2[i] = A[i+j];
            A3[i] = A[n/2 + j];
            A4[i] = A[j];
        }
    }
	
    b1 = foo(A1);
    b2 = foo(A2);
    b3 = foo(A3);
    b4 = foo(A4);
}
\end{lstlisting}

Her kan vi ikke like lett se løsninga siden funksjonen er rekursiv. Vi går sakte gjennom hva programmet gjør og forsøker å sette opp en funksjon for kjøretiden. Vi ser at \mono{foo} har to nestede for-løkker, hver av dem går $ n/2 $ ganger. Vi har derfor at hver gang vi kommer til denne løkka blir kjøretiden $ T_{\text{løkke}}(m) = (m/2)^2 $. Mot slutten av programmet har vi fire rekursive kall. Alle kallene kjører \mono{foo} med input av lengde $ n/2 $. Vi kan dermed sette opp en funksjon for kjøretiden:

\[ T(n) = C + 4\left(\frac{n}{2}\right)^2 + 4\,T\left(\frac{n}{2}\right) \]

\noindent der $ C $ er en konstant.  Vi kan sette inn for $ T(n/2) $:

\[ T(n) = C + 4\left(\frac{n}{2}\right)^2 + 4\,\left(C + 4\left(\frac{n/2}{2}\right)^2 + 4\,T\left(\frac{n/2}{2}\right)\right)  \]

Igjen kan vi sette inn for $ T(n/2) $, og slik kan vi fortsette. Siden vi halverer $ n $ hver gang ser vi at vi må gjøre dette $ \log_2(n) $ ganger før vi får at $ n=1 $, og rekursjonen stoppes av den øverste if-testen. Vi ser at i hvert rekursjonssteg så gjør vi $ O(n^2) $ operasjoner, og siden vi gjør $ O(\log_2 n) $ rekusjonssteg, får vi at
\[ T(n) = O(n^2 \log_2 n) \]
\end{example}