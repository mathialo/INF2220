\section{\color{red}Sortering}
\subsection{Formaliteter}
\label{sort_form}
Før vi kan begynne å se på noen spesielle sorteringsalgoritmer må vi formalisere hva vi mener med sortering. Vi definerer sortering slik:

\begin{definisjon}
Anta at \verb|a[]| er en liste av sammenlignbare elementer. Vi sier at \verb|a'[]| er den tilhørende sorterte lista hvis følgende kriterier er oppfylt:
\begin{enumerate}[i]
\item \verb|a'[i]| $ < $ \verb|a'[i+1]| for alle $ \verb|i| = 0, 1, \ldots, n-1 $
\item Alle elementene i \verb|a[]| er med i \verb|a'[]|
\end{enumerate}
\end{definisjon}

Det andre kriteriet kan virke litt snodig, men uten det ville sortering vært veldig enkelt. Vi kunne i så fall bare generert en ny liste med elementer i sortert rekkefølge, og det første kriteriet ville vært oppfylt. Vi trenger derfor bevaringskriteriet. 

Med ``sammenlignbare'' mener vi at det finnes en måte å entydig bestemme om et element er større enn, mindre enn eller lik et annet element. Hvis vi skal sortere tall er jobben enkel: vi sammenligner numerisk verdi. Hvis vi skal sammenligne to tekststrenger er det ikke like opplagt hvordan vi skal gjøre det. Skal vi sortere alfabetisk? Etter lengde på ordet? I et sånt tilfelle er det opp til oss å velge et fornuftig sammenligningskriterie. Det er vilkårlig hvordan vi sammenligner to elementer, så lenge vi gjør det likt gjennom hele sorteringen. 

\subsubsection{\color{red}paradigmer (?) og begreper}
\label{sec:sortbegrep}

\paragraph{Klasser av sorteringsalgoritmer}
Avhengig av hvordan de virker, deler vi sorteringsalgoritmer inn i to klasser:
Sammenligningsbaserte og Verdibaserte. Førstnevnte baserer seg på å sammenligne
to og to verdier i arrayen, mens de verdibaserte finner den riktige plasseringen
til hvert element kun ut ifra verdien på elementet. 


\paragraph{\color{red}Stabil Sortering}\label{stabil}
Noen ganger ønsker vi å sortere etter flere kriterier. Et eksempel kan være
epost, der vi f.eks. vil sortere både etter avsender og alfabetisk, slik at vi
får at alle som er fra samme avsender står alfabetisk i forhold til hverandre.

\begin{definisjon} Stabil Sortering
  Dersom en algoritme er \textbf{stabil}, er det et krav at innbyrdes rekkefølge
  etter sortering. Dvs at dersom \verb|a[i] = a[j]| og\verb|i < j|, og hvis
  \verb|a[i]| er sortert på plass \verb|k| i \verb|a'[k]| og \verb|a[j]| sortert
  på plass \verb|r| i \verb|a[]|, så skal \verb|k<r| 
\end{definisjon}

Følgende algoritmer (altså ikke alle) gjennomgått i kurset oppfyller kravet om
stabil sortering:
\begin{itemize}
\item \nameref{insertsort} 
\item \nameref{radixsort} 
\item \nameref{mergesort}: Kommer an på implementasjon \color{red}Si noe om vår.
\end{itemize}


\paragraph{Nedre grense for enkle sorteringsalgoritmer}
\begin{teorem} (7.1 i læreboka): Det gjennomsnittlige antall inversjoner i en
  array med $n$ elementer er $\frac{n(n-1)}{4}$
\end{teorem}

\begin{bevis}
  La $L$ være en array og $L_r$ den reverserte. Et hvilket som helst ordnet par
  i $L$ vil være en inversjon i enten $L$ eller $L_r$. Det totale antallet slike
  par i $L$ og $L_r$ vil være $\frac{n(n-1)}{2}$, altså har en gjennomsnittlig
  liste halvparten så mange, altså $\frac{n(n-1)}{4}$ 
\end{bevis}

\begin{teorem} (7.2 i læreboka): 
  $\frac{n(n-1)}{4}$
\end{teorem}





\subsection{\color{red}Noen algoritmer}
Vi skal nå se på noen konkrete sorteringsalgoritmer. Gjennomgående i alle
eksempler vil vi sortere tall etter tallverdi, men som diskutert i~\ref{sort_form} vil vi enkelt kunne tilpasse algoritmene til å sortere på andre kriterier. 


\subsubsection{\color{red}Boblesortering}
\label{bubblesort}
\textbf{Boblesortering} (\textbf{bubble/ (sinking) sort})  er en veldig ineffektiv sorteringsalgoritme, og brukes derfor lite i den virkelige verden. 
Idéen bak er derimot forholdsvis enkel:
Vi parvis sammenligner naboer i lista, og bytter om plassene deres dersom de står på feil plass.

\paragraph*{Wikipedias utgave}\label{sec:bubble-wiki}
Wikipedias versjon av boblesortering er forskjellig fra den som er gjennomgått i forelesning.
Her flytter vi oss hele tiden oppover i lista, og begynner på nytt når vi har kommet til slutten.
Dette gjentas helt til lista er ferdig sortert.

\begin{eks} Sortering av (5 1 2 5 8) \newline
  \textbf{Første gjennomgang}
  \begin{align*}
    (\mathbf{5}~ \mathbf{1}~ 2~ 5~ 8) \rightarrow (\mathbf{1}\, \mathbf{5} ~ 2 ~ 5 ~ 8) &\quad \text{Bytter, siden 5 > 1} \\
    (1~ \mathbf{5}~ \mathbf{4}~ 2~ 8) \rightarrow (1 ~ \mathbf{4} ~ \mathbf{5} ~ 2 ~ 8) &\quad \text{Bytter, siden 5 > 4} \\
    (1~ 4~ \mathbf{5}~ \mathbf{2}~ 8) \rightarrow (1~ 4~ \mathbf{2}~ \mathbf{5}~ 8)     &\quad \text{Bytter, siden 5 > 2} \\
    (1~ 4~ 2~ \mathbf{5}~ \mathbf{8}) \rightarrow (1~ 4~ 2~ \mathbf{5}~ \mathbf{8})     &\quad \text{Bytter ikke, siden 5 < 8}
  \end{align*}
  Deretter går vil tilbake til start og gjentar prosessen, helt til vi har gått gjennom lista uten at noen elementer har byttet plass.
  I dette tilfellet kreves tre gjennomganger.
\end{eks}

\javaimport{kode/bubbleSort.java}

\paragraph*{Utgaven presentert i forelesning}\label{sec:bubbleoptimal}
I forelesningen ble en noe alternativ utgave av boblesortering presentert, og det kan være lurt å forholde seg til denne, siden det er denne som er pensum.
Denne utgaven ligner veldig på innstikksortering, fordi den ``bobler'' et element som står på feil plass nedover i lista helt til den står på riktig plass.

\javaimport{kode/optBubbleSort.java}

\paragraph{Kompleksitet}
Merk at selv om utgaven av boblesortering presentert i forelesning i mange tilfeller kan være
betydelig raskere, vil disse to algoritmene ha samme kompleksitet.

\subparagraph{Worst Case: $O(n^2)$} Dersom vi ser for oss at vi skal sortere en
liste som er den reverserte av den sorterte, må vi gå gjennom lista $n$ ganger.
Dette fordi vi plasserer det største elementet i første gjennomgang, nest
største i andre gjennomgang osv. Det minste elementet behøver vi ikke å flytte,
men til slutt vil vi gå gjennom lista en gang for å sjekke at den er sortert.
Altså gjør vi $n(n-1)$ sammenligninger. $O(n(n-1)) = O(n^2 )$

\subparagraph{Best Case: $O(n)$} En allerede sortert liste: Vi sammenligner alle nabo-par i arrayen, og
siden vi ikke foretar noen bytter, vil algoritmen terminere etter å ha gjort
$(n-1)$ sammenligninger. Altså $O(n-1) = O(n)$

\subparagraph{Average Case: $O(n^2)$} 


\subsubsection{\color{red}Innstikksortering}\label{insertsort}
Dette er også en veldig enkel sorteringsalgoritme, og den er faktisk best for $n<50$
Idéen her er at elementene vi hittil har besøkt, er sorterte, isolert sett.
Deretter tar vi neste element, ``skyver'' de foregående elementene oppover,
helt til det nye elementet står på riktig plass.
Altså kan denne metoden minne mye om den~\hyperref[sec:bubbleoptimal]{optimaliserte boblesorteringen}. 

\javaimport{kode/insertionSort.java}

Merk at if-testen ikke trengs for at algoritmen skal fungere.
Faktisk lønner det seg å ikke ha med denne testen.
Siden if-tester er relativt dyre operasjoner, lønner det seg å ``plukke opp'' et
element, for deretter å sette den ned igjen på samme plass, enn å sjekke om det
står på feil plass.


\subsubsection{\color{red}Mergesort}\label{mergesort}

\subsubsection{\color{red}Heapsort}\label{heapsort}
I heapsort utnytter vi egenskapene til en heap.
Her bruker vi en max-heap, som vi setter alle elementene inn i. Da vet vi at
rota er det største elementet per detfinisjon av heap. Dersom vi så tar ut et og
et element av heapen, og plasserer på den bakerste ikke-opptatte plassen i
resultat-arrayen, vil vi få arrayen ut i sortert form.

\subsubsection{\color{red}Tresortering}\label{treesort}
I likhet med \nameref{heapsort} utnytter vi her egenskapene til en datastruktur,
nemlig binære søketrær. I et slikt tre vet vi at venstre barn til en node er
mindre enn noden selv, og at høyre barn er større eller lik noden.

Deretter traverserer vi treet i infix-rekkefølge. På den måten vil vi altså gå
gjennom treet i stigende rekkefølge, og vi har dermed sortert arrayen.

\subsubsection{\color{red}Quicksort}
\label{quick}
Som navnet antyder, er dette en ganske rask algoritme.
Idéen er at vi velger et (vilkårlig) element i arrayen (pivotelement), og ordner resten av
elementene slik at de som er mindre pivotelementet står på venstre side, og de
som er større på venstre. Deretter gjentar vi denne prosessen for disse to
subarrayene på hver side av pivotelementet.

\javaimport{kode/quickSort.java}

Som man kan se i koden ovenfor, er det litt problematisk å foreta selve
flyttingen av elementene i forhold til pivoten når vi jobber med arrayer, siden
vi ikke kan dele opp og sette sammen arrayer. En detaljert beskrivning (og
illustrering) av tankegangen finnes i boka i seksjon 7.7.2 (side 312). 
En noe forenklet utgave følger her:
\begin{enumerate}
\item Bytt plass på pivoten og det siste elementet, slik at vi unngår at den
  kommer i veien for oss.
\item La $i$ og $j$ starte på henholdsvis første og \textbf{nest siste} plass i
  arrayen. 
\item\label{item:bevege} La $i$ bevege seg oppover, og $j$ nedover, helt til $i$ står på et
  element som er større enn pivotelementet, og $j$ står på et element som er
  mindre.
\item\label{item:bytt} Bytt plass på elementene som $i$ og $j$ peker på.
\item Gjenta~\ref{item:bevege} og~\ref{item:bytt} helt til $i$ og $j$ har
  passert hverandre. Nå peker $j$ på et element mindre enn pivot, og $i$ på et
  som er større. Dersom vi nå bytter plass på verdien på $i$ og pivoten.
\end{enumerate}

\paragraph{Om å velge pivot}
Hvilket element vi velger som pivotelement kan ha veldig mye å si for kjøretiden
til algoritmen. Dersom vi får en array som allerede er sortert, og vi velger det
første elementet (altså det minste) som pivotelement, vil vi bruke kvadratisk
tid på å gjøre egentlig ingenting!

Det beste valget av pivot, villet vært medianen. Da vil størrelsen på
subarrayene bli det samme (med en forskjell på 1 hvis vi har partall antall
elementer). Dessverre vil det å regne ut medianen til en stor array ta alt for
lang tid. Boka foreslår en mulig løsning ved å trekke tilfeldige tall, og
deretter bruke medianen til disse som pivot. Det tilfeldige elementet viser seg
imidlertid å hjelpe lite. Derfor blir det til slutt anbefalt å bruke medianen
til det første, siste og midterste elementet i arrayen som pivot.



\subsubsection{\color{red}Radixsort}\label{radixsort}
Radix sort sorterer en array på et siffer av gangen.
Metoden forekommer i to varianter, en fra høyre til venstre, og en som gjør
motsatt. Generelt for begge algoritmene har vi
\begin{enumerate}
\item Finn max verdi i arrayen. (Største siffer i alle tallene)\color{red}??
\item\label{item:tell} Tell opp hvor mange elementer det er av hver verdi av det sifferet vi ser
  på. (hvor mange 0-er, 1-ere osv)
\item Ved å addere disse antallene, finner vi ut hvor i arrayen tallene skal stå
  ved å addere verdiene i arrayen vi lagde i~\ref{item:tell}, men starte på 0.
\end{enumerate}
En spesiell ting ved Radix sort er at den aldri gjør sammenligninger mellom to
tall.

\paragraph{\color{red}Right-Radix (RR)}
Den ``vanlige'' formen for Radix-sort, og er iterativ.
Den begynner med det minst gjeldende sifferet (det bakerste). Etter
``fellesstegene'' blir den siste arrayen vi lagde brukt til å slå opp hvor vi
skal plassere elementet. Vi går gjennom arrayen fra venstre til høyre, og setter
inn elementer i den nye arrayen. At vi gjør det fra høyre til venstre er viktig,
siden vi ønsker en~\hyperref[stabil]{stabil} sortering. Etter at vi har plassert
et element på sin plass i den nye ``sorterte'' arrayen, bør vi øke verdien vi
akkurat brukte i telle-arrayen med 1 så vi lett kan finne ut hvor neste tall med
samme siffer på gjeldende plass. Dette gjentar vi helt til vi har vært gjennom
maks antall gjeldende siffer.



\paragraph{\color{red}Left-Radix (LR)}
{\color{red}Er dette i det hele tatt pensum?}


\subsection{\color{red}Parallell Sortering}

\subsubsection{Amdahls lov}
\label{sec:amdahl}

Amdahls lov gir oss en slags øvre grense for hvor mye tid vi kan spare på å
parallellisere en algoritme.
\begin{teorem}\textbf{Amdahls lov: } 
  Anta at du har en algoritme der en andel
  $p\%$ må kjøres sekvensielt, men at resten kan gjøres i parallell.
  Med $k$ lik antall prosessorer, får vi at maksimal speedup er
  \[
    S = \frac{\text{Sekvensiell tid}}{\text{Parallell tid}} \leq \frac{100}{p + \frac{100 - p}{k}}
  \]
\end{teorem}

Denne loven har imidlertid møtt motstand i Gustafson, som hevder at andelen av
programmet som er sekvensielt blir mindre etterhvert som problemet blir større.
Altså er ikke Amdahls lov fast.

